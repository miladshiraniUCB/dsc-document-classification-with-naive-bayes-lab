{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with Naive Bayes - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, you'll practice implementing the Naive Bayes algorithm on your own.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will:  \n",
    "\n",
    "* Implement document classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset\n",
    "\n",
    "To start, import the dataset stored in the text file `'SMSSpamCollection'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"SMSSpamCollection\", sep = \"\\t\", names = [\"label\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account for class imbalance\n",
    "\n",
    "To help your algorithm perform more accurately, subset the dataset so that the two classes are of equal size. To do this, keep all of the instances of the minority class (spam) and subset examples of the majority class (ham) to an equal number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spam': 0.5, 'ham': 0.5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "minority_spam = df[df[\"label\"] == \"spam\"]\n",
    "majority_ham = df[df[\"label\"] == \"ham\"].sample(n = len(minority_spam))\n",
    "df_equal = pd.concat([minority_spam, majority_ham], axis = 0)\n",
    "classes_m = dict(df_equal[\"label\"].value_counts(normalize = True))\n",
    "classes_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "Now implement a train-test split on the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_equal[\"text\"]\n",
    "y = df_equal[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 17)\n",
    "train_df = pd.concat([X_train, y_train], axis = 1)\n",
    "test_df = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word frequency dictionary for each class\n",
    "\n",
    "Create a word frequency dictionary for each class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5883"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "def wrod_frequency(data, corpus = False):\n",
    "    \n",
    "    if corpus == False:\n",
    "    \n",
    "        target = classes_m.keys()\n",
    "    \n",
    "        word_frequency = {}\n",
    "    \n",
    "        for label in target:\n",
    "        \n",
    "            line = data[data[\"label\"] == label]\n",
    "        \n",
    "            word_text_frequency = {}\n",
    "        \n",
    "            for index in line[\"text\"].index:\n",
    "            \n",
    "                text = line[\"text\"][index]\n",
    "            \n",
    "                for word in text.split():\n",
    "                \n",
    "                    word_text_frequency[word]= word_text_frequency.get(word, 0) + 1\n",
    "                \n",
    "            word_frequency[label] = word_text_frequency\n",
    "        \n",
    "        return word_frequency \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        vocabulary = set()\n",
    "#         vocabulary = 0\n",
    "\n",
    "        for line in data[\"text\"]:\n",
    "            \n",
    "#             bag = {}\n",
    "            \n",
    "            for word in line.split():\n",
    "            \n",
    "                vocabulary.add(word)\n",
    "                \n",
    "#                 bag[word] = bag.get(word, 0) + 1\n",
    "                \n",
    "#             vocabulary += sum(bag.values())\n",
    "        \n",
    "        \n",
    "#         return vocabulary  \n",
    "        return len(vocabulary)\n",
    "\n",
    "train_frequency = wrod_frequency(train_df, corpus = False)\n",
    "test_frequency = wrod_frequency(test_df,corpus = False)   \n",
    "vocabulary_train = wrod_frequency(train_df,corpus = True)   \n",
    "vocabulary_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the total corpus words\n",
    "Calculate V, the total number of words in the corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5883"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "def wrod_corpus(data):\n",
    "        \n",
    "    vocabulary = set()\n",
    "\n",
    "    for line in data[\"text\"]:\n",
    "            \n",
    "        for word in line.split():\n",
    "            \n",
    "            vocabulary.add(word)\n",
    "         \n",
    "    return len(vocabulary)\n",
    "\n",
    "train_frequency = wrod_frequency(train_df, corpus = False)\n",
    "test_frequency = wrod_frequency(test_df,corpus = False)   \n",
    "V_m = wrod_corpus(train_df)   \n",
    "V_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bag of words function\n",
    "\n",
    "Before implementing the entire Naive Bayes algorithm, create a helper function `bag_it()` to create a bag of words representation from a document's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_it_m(line):\n",
    "    \n",
    "    bag = {}\n",
    "        \n",
    "    for word in line.split():\n",
    "            \n",
    "        bag[word]= bag.get(word, 0) + 1\n",
    "                \n",
    "    return bag\n",
    "\n",
    "# bag_it_m(X_train[312])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Naive Bayes\n",
    "\n",
    "Now, implement a master function to build a naive Bayes classifier. Be sure to use the logarithmic probabilities to avoid underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# def classify_doc(doc, class_word_freq, classes, V, return_posteriors=False):\n",
    "\n",
    "def classify_doc_m(doc, class_freq, classes_m, V_m, return_posteriors=False):\n",
    "    \n",
    "    bag  = bag_it_m(doc)\n",
    "    \n",
    "    posteriors = []\n",
    "    \n",
    "    clss = []\n",
    "        \n",
    "    \n",
    "    for item in class_freq.keys():\n",
    "        \n",
    "        \n",
    "        p = np.log(classes_m[item])\n",
    "        \n",
    "        \n",
    "        for word in bag.keys():\n",
    "            \n",
    "            num = bag[word] + 1\n",
    "            \n",
    "            denum = class_freq[item].get(word, 0) + V_m\n",
    "            \n",
    "            p += np.log(num / denum)\n",
    "            \n",
    "#         print(p)\n",
    "            \n",
    "        clss.append(item)\n",
    "        \n",
    "        posteriors.append(p)\n",
    "        \n",
    "    if return_posteriors:\n",
    "            \n",
    "        print(posteriors)\n",
    "        \n",
    "    return clss[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your classifier\n",
    "\n",
    "Finally, test your classifier and measure its accuracy. Don't be perturbed if your results are sub-par; industry use cases would require substantial additional preprocessing before implementing the algorithm in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-200.4960104602396, -200.41192443826716]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "classify_doc_m(X_train[312], train_frequency, classes_m, V_m, return_posteriors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_hat = X_train.apply(lambda x: classify_doc_m(x, train_frequency, classes_m, V_m, return_posteriors=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.757143\n",
       "True     0.242857\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = y_train == y_train_hat\n",
    "diff.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ ME:\n",
    "\n",
    "check the solution on github provided below. The solution here is not that good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.757143\n",
       "True     0.242857\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', names=['label', 'text'])\n",
    "df.head()\n",
    "\n",
    "# Account for class imbalance\n",
    "minority = df[df['label'] == 'spam']\n",
    "undersampled_majority = df[df['label'] == 'ham'].sample(n=len(minority))\n",
    "df2 = pd.concat([minority, undersampled_majority])\n",
    "df2.label.value_counts()\n",
    "\n",
    "\n",
    "# p-classes\n",
    "p_classes = dict(df2['label'].value_counts(normalize=True))\n",
    "p_classes\n",
    "\n",
    "# Train-test split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X = df2['text']\n",
    "# y = df2['label']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
    "train_df = pd.concat([X_train, y_train], axis=1) \n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "# Create the word frequency dictionary for each class\n",
    "# Will be a nested dictionary of class_i : {word1:freq, word2:freq..., wordn:freq},.... class_m : {}\n",
    "class_word_freq = {} \n",
    "classes = train_df['label'].unique()\n",
    "for class_ in classes:\n",
    "    temp_df = train_df[train_df['label'] == class_]\n",
    "    bag = {}\n",
    "    for row in temp_df.index:\n",
    "        doc = temp_df['text'][row]\n",
    "        for word in doc.split():\n",
    "            bag[word] = bag.get(word, 0) + 1\n",
    "    class_word_freq[class_] = bag\n",
    "\n",
    "    \n",
    "# Count the total corpus words\n",
    "vocabulary = set()\n",
    "for text in train_df['text']:\n",
    "    for word in text.split():\n",
    "        vocabulary.add(word)\n",
    "V = len(vocabulary)\n",
    "V\n",
    "    \n",
    "# Create a bag of words function\n",
    "def bag_it(doc):\n",
    "    bag = {}\n",
    "    for word in doc.split():\n",
    "        bag[word] = bag.get(word, 0) + 1\n",
    "    return bag\n",
    "\n",
    "# Implementing Naive Bayes\n",
    "def classify_doc(doc, class_word_freq, p_classes, V, return_posteriors=False):\n",
    "    bag = bag_it(doc)\n",
    "    classes = []\n",
    "    posteriors = []\n",
    "    for class_ in class_word_freq.keys():\n",
    "        p = np.log(p_classes[class_])\n",
    "        for word in bag.keys():\n",
    "            num = bag[word]+1\n",
    "            denom = class_word_freq[class_].get(word, 0) + V\n",
    "            p += np.log(num/denom)\n",
    "        classes.append(class_)\n",
    "        posteriors.append(p)\n",
    "    if return_posteriors:\n",
    "        print(posteriors)\n",
    "    return classes[np.argmax(posteriors)]\n",
    "\n",
    "#Test your classifier\n",
    "import numpy as np\n",
    "\n",
    "y_hat_train = X_train.map(lambda x: classify_doc(x, class_word_freq, p_classes, V))\n",
    "residuals = y_train == y_hat_train\n",
    "residuals.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up (Optional)\n",
    "\n",
    "Rework your code into an appropriate class structure so that you could easily implement the algorithm on any given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Well done! In this lab, you practiced implementing Naive Bayes for document classification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
